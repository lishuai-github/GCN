# 预训练模型论文
## 1.Pre-Trained Models:Past,Present and Future
清华大学，中国人民大学，复旦大学

### **介绍**
深度神经网络因为有很多的参数，所以需要充足的数据来进行训练防止过拟合和低泛化能力。
人工构建高质量的数据集需要消耗很多的人力财力物力。而且，有些标注需要专家来进行，有些任务需要的数据量非常巨大。因此，很长一段时间以来，“如何使用有限的人工标注的数据集训练出有效的神经网络？”是个重点研究问题。

一个里程碑事件是迁移学习(Transfer learning)的提出。预训练阶段从一个或多个源任务中获取知识，微调阶段将学习到的只是迁移到目标任务中。因为已经从预训练阶段获取了大量的知识，所以模型可以在微调阶段使用有限的样本来很好的处理目标任务。
### **背景**
**迁移学习和有监督的预训练**

迁移学习是从多种源任务中捕捉知识然后将知识应用到目标域上。

两种迁移方法：特征迁移和参数迁移。
特征迁移方法预训练有效的特征表示来对跨领域和跨任务的知识进行预编码。参数迁移的方法假设源任务和目标任务共享模型参数和超参数的先验分布。将知识预先编码到共享的模型参数中，然后在目标任务上微调预训练的模型参数。