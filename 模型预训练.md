# 预训练模型论文
## 1.Pre-Trained Models:Past,Present and Future
清华大学，中国人民大学，复旦大学

### **介绍**
深度神经网络因为有很多的参数，所以需要充足的数据来进行训练防止过拟合和低泛化能力。
人工构建高质量的数据集需要消耗很多的人力财力物力。而且，有些标注需要专家来进行，有些任务需要的数据量非常巨大。因此，很长一段时间以来，“如何使用有限的人工标注的数据集训练出有效的神经网络？”是个重点研究问题。

一个里程碑事件是迁移学习(Transfer learning)的提出。预训练阶段从一个或多个源任务中获取知识，微调阶段将学习到的只是迁移到目标任务中。因为已经从预训练阶段获取了大量的知识，所以模型可以在微调阶段使用有限的样本来很好的处理目标任务。
### **背景**
**迁移学习和有监督的预训练**

迁移学习是从多种源任务中捕捉知识然后将知识应用到目标域上。

两种迁移方法：特征迁移和参数迁移。
特征迁移方法预训练有效的特征表示来对跨领域和跨任务的知识进行预编码。参数迁移的方法假设源任务和目标任务共享模型参数和超参数的先验分布。将知识预先编码到共享的模型参数中，然后在目标任务上微调预训练的模型参数。

词嵌入使用的是特征迁移方法，
**自监督学习和自监督的预训练**
自监督学习可以被视为无监督学习的一种，因为都是使用的无标签样本。无监督学习的目标是检测数据模式，而自监督仍然是在监督的范式中。自监督的预训练在NLP领域中取得了重大进展。
### Transformer和有代表性的PTMs
最近PTMs成功的关键是整合了自监督学习和Transformer。
**Transformer**
self-attention
**GPT**
GPT擅长自然语言生成。PTMs包括预训练和微调两个阶段。
以Transformer的解码器为骨架，经过生成式的预训练和有判别的微调。它结合了Transformer和自监督的预训练。
适合于自然语言推理，问答、常识推理、语义相似性和分类。
通过将所有的单词之前的单词作为上下文，最大化所有单词的条件概率。
在预训练阶段每个单词的条件概率通过Transformer来计算，使用的是多头自注意力操作前面的单词。
**BERT**
BERT采用自编码语言建模，而不是GPT中使用的自回归语言建模。

### 4. 设计有效的架构
4.1统一序列模型
NLP下游任务可以分为：自然语言理解，开放式语言生成，非开放式语言生成。
但任务之间的边界时模糊的。
**多模态预训练**
现存的预训练模型更多的关注改进模型架构，使用更多的数据，，设计更好的预训练任务。
对于基于图像-文本的PTMs，目前的研究大多基于视觉-语言BERT体系结构。主要的挑战在于视觉内容和文本内容在统一的语义空间中的对齐。为此，主要有两种模型架构设计:双流和单流。双流模型：ViLBERT（将图片和文本分别输入通过设计的联合注意力transformer块进行融合），LXMERT(分别处理两个模态，然后通过交叉模态编码器进行融合)。单流模型：将特征融合之后输入一个transformer中，如VisualBERT, Unicoder-VL,B2T2。

**提升精算性能**
使用float16进行运算。混合进度训练方法。巧妙使用CPU内存。
数据并行。



## When Old Meets New: Emotion Recognition from Speech Signals

从语音信号中提取MFCC特征和语谱图，使用ResNet50对语谱图进行特征提取，融合MFCC特征输入LSTM和SVM进行离散情感分类。
